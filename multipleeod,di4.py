# -*- coding: utf-8 -*-
"""MultipleEOD,DI4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18mI5jyaEdl0TlOC29Cw_E2Bsw2p5NNEr
"""

# Step 1: Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import r2_score, mean_squared_error

# Step 2: Load dataset
df = pd.read_csv("Gender_loan.csv")  # adjust path if needed
print("Dataset loaded successfully!")
print(df.head())

# Step 3: Encode categorical columns (if any)
# We'll automatically encode all object-type columns
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])

print("\nEncoded dataset preview:")
print(df.head())

# Step 4: Separate features (X) and target (y)
# Assuming the target column is named 'bank_loan' or similar
target_col = 'bank_loan'
X = df.drop(columns=[target_col])
y = df[target_col]

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train Linear Regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Step 7: Make predictions
y_pred = model.predict(X_test)

print("\nModel training complete!")
print(f"Number of features: {X_train.shape[1]}")

# Step 8: Evaluate model performance
acc = accuracy_score(y_test, y_pred)
print(f"Classification Accuracy: {acc:.4f}")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

protected_attr = 'sex'

from sklearn.metrics import precision_score
from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate

# Extract sensitive feature from test set
sensitive_test = X_test[protected_attr]

# Create MetricFrame to evaluate per group
metrics = {
    'TPR': true_positive_rate,
    'Selection Rate': selection_rate,
    'Precision': precision_score
}

mf = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_test)
print("Metrics by group:\n", mf.by_group)

# 1. Equal Opportunity Difference
equal_opp_diff = mf.difference(method='between_groups')['TPR']

# 2. Disparate Impact Ratio (80% rule)
dir_ratio = mf.ratio(method='between_groups')['Selection Rate']

# 3. Predictive Parity Difference
predictive_parity_diff = mf.difference(method='between_groups')['Precision']

print("\nFairness Metrics:")
print(f"Equal Opportunity Difference: {equal_opp_diff:.4f}")
print(f"Disparate Impact Ratio: {dir_ratio:.4f}")
print(f"Predictive Parity Difference: {predictive_parity_diff:.4f}")

'''
The fairness evaluation confirms the model achieves reasonable ethical performance across
genders.
Although slight differences exist they are within acceptable boundaries.

Equal Opportunity Diff (−0.045): Slightly favors males.
• Disparate Impact (0.916): Indicates acceptable fairness level.
• Predictive Parity Diff (0.098): Small positive difference — female predictions slightly
more precise.
This indicates minor bias but no critical fairness violations.

import matplotlib.pyplot as plt

metrics_names = ['Equal Opportunity Difference', 'Disparate Impact Ratio', 'Predictive Parity Difference']
metrics_values = [equal_opp_diff, dir_ratio, predictive_parity_diff]

plt.figure(figsize=(10, 6))
plt.bar(metrics_names, metrics_values, color=['skyblue', 'lightcoral', 'lightgreen'])
plt.ylabel('Value')
plt.title('Fairness Metrics Visualization')
plt.ylim(min(0, min(metrics_values) - 0.1), max(1, max(metrics_values) + 0.1))
plt.axhline(y=1, color='gray', linestyle='--', label='Ideal Ratio (for DI)')
plt.axhline(y=0, color='gray', linestyle='--', label='Ideal Difference (for EOD, PPD)')
plt.legend()
plt.grid(axis='y', linestyle='--')
plt.show()

