# -*- coding: utf-8 -*-
"""Privacy-acc-tradeoff9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1USPzuQkfnguL56oazICIzsT34wQzRLOM
"""

#Privacy Accuracy Trade-off in ML Models

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# 1. Load and preprocess data
data = pd.read_csv("Loan.csv")

le = LabelEncoder()
for col in ['Gender', 'Education', 'Married']:
    data[col] = le.fit_transform(data[col].astype(str))

# Impute missing values before selecting features for X
data['LoanAmount'].fillna(data['LoanAmount'].median(), inplace=True)
data['Credit_History'].fillna(data['Credit_History'].median(), inplace=True)

X = data[['ApplicantIncome', 'Gender', 'Education',
          'Credit_History', 'LoanAmount']]
y = data['Loan_Status']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42
)

# 2. Baseline model (no differential privacy)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

baseline_acc = accuracy_score(y_test, model.predict(X_test))
print("Baseline (No DP) Accuracy:", round(baseline_acc, 4))

# 3. Simulated Differential Privacy by adding Laplace noise
def add_noise(X, eps):
    scale = 1 / eps
    noisy = X + np.random.laplace(0, scale, X.shape)
    return noisy

epsilons = [0.1, 0.5, 1.0, 2.0]
accuracies = []

for eps in epsilons:
    X_train_noisy = add_noise(X_train, eps)
    dp_model = LogisticRegression(max_iter=1000)
    dp_model.fit(X_train_noisy, y_train)

    dp_acc = accuracy_score(y_test, dp_model.predict(X_test))
    accuracies.append(dp_acc)
    print(f"Epsilon {eps} → Accuracy: {dp_acc:.4f}")

# 4. Plot privacy accuracy trade-off
plt.figure(figsize=(7,4))
plt.plot(epsilons, accuracies, marker='o', color='blue', label='DP Model')
plt.axhline(baseline_acc, color='red', linestyle='--', label='No DP baseline')

plt.title("Privacy Accuracy Trade-off in Differentially Private Model")
plt.xlabel("Epsilon (ε)")
plt.ylabel("Model Accuracy")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

'''
Increasing ε → Less noise → Higher accuracy (but weaker privacy).

The results confirm that:
• At ε = 0.1, privacy is strongest but accuracy drops (~69%).
• At ε = 1.0, privacy and accuracy reach an ethical balance (~80%).
• At ε = 2.0, accuracy (~83%) almost matches baseline, showing minimal privacy
protection.
Hence, Differential Privacy provides a configurable framework for protecting user data
during model training while maintaining acceptable utility, a key requirement for ethical and
trustworthy AI.